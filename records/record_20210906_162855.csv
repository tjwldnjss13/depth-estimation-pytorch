optimizer, epoch, lr, loss(train), loss, loss(ap), loss(ds), loss(lr)
Adam, 1, 0.0001, 4.42130, 4.07676, 3.02562, 0.03912, 1.01202
Adam, 2, 0.0001, 3.57514, 3.38658, 2.99956, 0.03685, 0.35017
Adam, 3, 0.0001, 3.20813, 3.31825, 2.98322, 0.03416, 0.30087
Adam, 4, 0.0001, 3.11105, 3.16939, 2.97517, 0.03265, 0.16157
Adam, 5, 0.0001, 3.06529, 3.13194, 2.97018, 0.03083, 0.13093
Adam, 6, 0.0001, 3.04546, 3.10386, 2.96504, 0.02947, 0.10934
Adam, 7, 0.0001, 3.03040, 3.08771, 2.96087, 0.02862, 0.09821
Adam, 8, 0.0001, 3.01807, 3.07726, 2.95651, 0.02740, 0.09335
Adam, 9, 0.0001, 3.00447, 3.07159, 2.94981, 0.02607, 0.09571
Adam, 10, 0.0001, 2.98171, 3.03858, 2.93580, 0.02624, 0.07654
Adam, 11, 0.0001, 2.97143, 3.03488, 2.93185, 0.02551, 0.07753
Adam, 12, 0.0001, 2.96131, 3.01631, 2.92727, 0.02525, 0.06379
